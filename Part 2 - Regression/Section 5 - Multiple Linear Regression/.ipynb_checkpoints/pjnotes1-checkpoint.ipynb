{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple lin regression\n",
    "assumptions of linear regression:<br>\n",
    "linearity<br>\n",
    "homoscedasticity<br>\n",
    "multivariate normality<br>\n",
    "independence of errors<br>\n",
    "lack of multicollinearity<br>\n",
    "<p>\n",
    "\n",
    "\n",
    "<b>Dummy variables</b>\n",
    "y = b0 + b1*x1 + b2*x2 + b3*x3 + the feature that doesn't have a numerical value-->categorical variable(for this we use(create) dummy variable) for every value we use new column.(this will be b4*D1(the dummy variable))<br>\n",
    "we can't include all of the dummy variables\n",
    "<p>\n",
    "\n",
    "for 2 dummy variables D1 and D2, D2 =1 - D1 \n",
    "<br>\n",
    "and this makes the D2 get included in b0 when D1 is 0 and D2 is 1. if we include both D1 and D2 in the lin regression model, the model won't be able to distinguish between the effects of D1 and D2 so that will make a weaker model.<br>\n",
    "we use one less dummy variables than required.\n",
    "<b> This is Dummy variable trap</b><br><p>\n",
    "\n",
    "For multiple features, we have to decide which ones to keep and which ones to get rid of while designing the model. we do this to avoid gigo and also the practicality of the model and selecting the right features to consider.<br>\n",
    "<p>\n",
    "Methods to do so are as follows:<br>\n",
    "    1. All-in<br>\n",
    "    2.Backward Elimination<br>\n",
    "    3.Forward elimination<br>\n",
    "    4. Bidirectional eliminaiton<br>\n",
    "    5.Score comparision<br>\n",
    " 2,3,4 are also known as stepwise regression or just 4.<br>\n",
    "    \n",
    "1.all in when we have prior knowledege, you have to or preparing for backward elimination.<br>\n",
    "\n",
    "2. Backward elimation. set significance level say 0.05 fit model with all possible predictors then if predictor with highest p value P> SL---> remove the predictor and if none like that found, finish the model.\n",
    "\n",
    "<i>P-values are the probability of obtaining an effect at least as extreme as the one in your sample data, assuming the truth of the null hypothesis.</i><p>\n",
    "    \n",
    "3. Forward selection: set the significance level say 0.05 and fit simple reg models with y~xn select the one with lowest p-value and then keep this variable and fit all possible with one extra predictor added to the one(s) you already have. then consider with the lowest P-value. if P<SL...do it again or else finish.<p>\n",
    "\n",
    "\n",
    "4. Bidirecitonal elimination: select SL perform the next step of FS(new variables must have P<SL to enter) then perforl all steps of BE (old variables must have P<SL) then till no new variables can enter and no old variables can exit.\n",
    "\n",
    "5. score comparision.\n",
    "\n",
    "here we will use the backward elimination.<p>\n",
    "    \n",
    "    To do the backward elimination for the model we use \n",
    "<b>import statsmodels.formula.api as sm</b><br>\n",
    "\n",
    "to make the model consistent with y = b0 + b1*x1 + b2*x2 + b3*x3 +...+ bn*xn we need to add the x0---> vector of 1's to make the model consistent to y = b0*x0 + b1*x1 + b2*x2 + b3*x3 +...+ bn*xn <p>\n",
    "we select X_opt = X[:,[0,1,2,3,4,5]] \n",
    "1. we use sm.OLS() for the ordinary least square it takes endog = y, exog= X_opt: <br>\n",
    "    <b>regressor_OLS = sm.OLS(endog = y, exog= X_opt).fit()</b>\n",
    "2. for p values we use regressor_OLS.summary() as p-value is probabiliy.\n",
    "    the lower the p value, the more significant the independent variable is going to be wrt dependent variable. we find the highest p-value and compare with the Significant Level say 5% and thus if more we remove it.\n",
    "    \n",
    "3. refit the model without that particular feature.\n",
    "<b>X_opt = X[:,[0,1,3,4,5]]</b>\n",
    "<b>regressor_OLS = sm.OLS(endog = y, exog= X_opt).fit()</b>\n",
    "and re check for the summary and follow the steps\n",
    "\n",
    "sometimes we see the features with very low p-values almost very near to the SL value 0.05, so we look at other metrics like <b>r sqared</b> and <b>adjusted r squared</b> to select the proper features.\n",
    "<p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
