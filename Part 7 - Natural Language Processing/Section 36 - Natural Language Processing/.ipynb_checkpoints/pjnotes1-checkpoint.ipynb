{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Natural Language Processing (or NLP) is applying Machine Learning models to text and language. Teaching machines to understand what is said in spoken and written word is the focus of Natural Language Processing. Whenever you dictate something into your iPhone Android device that is then converted to text, thatâ€™s an NLP algorithm in action.`\n",
    "\n",
    "`You can also use NLP on a text review to predict if the review is a good one or a bad one. You can use NLP on an article to predict some categories of the articles you are trying to segment. You can use NLP on a book to predict the genre of the book. And it can go further, you can use NLP to build a machine translator or a speech recognition system, and in that last example you use classification algorithms to classify language. Speaking of classification algorithms, most of NLP algorithms are classification models, and they include Logistic Regression, Naive Bayes, CART which is a model based on decision trees, Maximum Entropy again related to Decision Trees, Hidden Markov Models which are models based on Markov processes.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV vs TSV:\n",
    "comma separated values vs Tab separated values.<br>\n",
    "\n",
    "### We use the Data collected for the reviews for a restaurant to use NLP on it and find if a new review is +ve or negative. \n",
    "### Importing the data\n",
    "<b>we can't use the CSV as commas can't be used as delimeters to separate the data, as commas can be part of review too. So we use the TSV format for this</b><br>\n",
    "now for that we use same `pd.read_csv('',delimiter='\\t',quoting=3)` with the delimiter tab.<br>\n",
    "and to ignore the double quotes in the reviews, we use `quoting=3` parameter.<br><p>\n",
    "\n",
    "<b>We can use NLP for not just reviews but also for articles, written speech or some discourse. Discourse analysis is a part of NLP. </b>\n",
    "\n",
    "### Cleaning the data:\n",
    "\n",
    "<b>Need for this is that we need only the relevant words to make the bag of words</b> <br>\n",
    "we will get rid of articles, numbers, remove ..., use stemming ie. to use roots for the words that mean same thing. We will get rid of the capitals.<br>\n",
    "Finally we will make <b>a bag of words model</b> out of it which is <b>Tokenizing process</b>. i.e it splits texts into words.. `relevant words` and then we will attribute one coulumn for each words and for each review, each column will contain the number of times the associated word appears in the review. Hence lots of 0's and some 1's and less 2's and 3's. <b>Sparse matrix(to make the sparse matrix have lower dimentions(excluding the signs, numbers and stopwords we decrease the dimensions of the sparse matrix that we have to deal with)  )</b> will be created by this. <br><p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing the dataset (The TSV file, Tab separated values)\n",
    "\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv',delimiter='\\t',quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow... Loved this place.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We will be removing anything except the letters first and replacing it with space for the 1st review for example:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "review = re.sub('[^a-zA-Z]',' ',dataset['Review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow    Loved this place '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lets remove the capital letters:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow    loved this place '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = review.lower()\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Let's remove the words which are not very relevant to the ML algo: (words like the, a, ...)<i>We use the NLTK lib and tools to achive our goal. list of words contains the list of irrelavant words. For this we use `nltk.download('stopwords')`</i> Now we check for every review and if any stopwords are found, we delete it from the review</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/prashant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow', 'loved', 'this', 'place']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = review.split()\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Here 'this' is the stopword, we remove it we convert the list of stopwords to set of stop words to make the process faster<b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "#review\n",
    "#['wow', 'loved', 'place']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steming: (finding the roots for the words to make them represented by their roots):\n",
    "`from nltk.stem.porter import PorterStemmer`<br>\n",
    "is used to stem the words<br>\n",
    "`ps = PorterStemmer()\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow', 'love', 'place']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lets join the review</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow love place'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>we perform cleaning for all the reviews, and then put em in the list corpus<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the bag of words:\n",
    "We take the words in all the reviews without repeating them, and then we make a sparse matrix out of it with columns being the unique words itself.<b>Tokenization</b><br>\n",
    "filling the the rows with every time in a review the word appears we fill 1 else 0.<br>\n",
    "<p>\n",
    "    Then we train the model on basis of sparse matrix and outcome then finally it will get us to classification problem for binary outcome. <p>\n",
    "        \n",
    "sklearn.feature_extraction.text---> CounterVectorizer\n",
    "<br>\n",
    "`cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()`\n",
    "toarray()---> converts it into a matrix.<br>\n",
    "X is now our sparse matrix.<br>\n",
    "\n",
    "for Countvectorizer(max_features=1500) this parameter will help us get the 1500 most frequent words.This reduces the sparsity and also give us the most relevant words for us to train our model on.<br>\n",
    "\n",
    "Also the dimentionality reduction can be used to make it simpler<br> \n",
    "<p>\n",
    "Now we use <b>Naive Bayes classification model</b> with X, and y to tain on.\n",
    "The model predicts the review with 73% accuracy.\n",
    "<br> \n",
    "<p>\n",
    "Again using <b>KNN classification model</b> with X, and y to tain on.\n",
    "The model predicts the review with 78% accuracy.\n",
    "<br> \n",
    "<p>\n",
    "Again using <b>SVM classification model</b> with X, and y to tain on.\n",
    "The model predicts the review with 89% accuracy.\n",
    "<br> \n",
    "<p>\n",
    "\n",
    "Again using <b>DT classification model</b> with X, and y to tain on.\n",
    "The model predicts the review with 90.5% accuracy.\n",
    "\n",
    "<br> \n",
    "<p>\n",
    "Again using <b>Random forest classification model</b> with X, and y to tain on.\n",
    "The model predicts the review with 85% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Language processing\n",
    "\n",
    "#Importing the libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing the dataset (The TSV file, Tab separated values)\n",
    "\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv',delimiter='\\t',quoting=3) \n",
    "\n",
    "#Cleaning the texts\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "for i in range(0,1000):\n",
    "    review = re.sub('[^a-zA-Z]',' ',dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "#Creating the Bag of Words model though the process of Tokenization.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:,1].values\n",
    "\n",
    "# Training a Classification model with the independent variable vector and dependent variable vector\n",
    "# We use Naive Bayes or DT or Random forest for NLP\n",
    "\n",
    "# Here we use Naive Bayes\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
